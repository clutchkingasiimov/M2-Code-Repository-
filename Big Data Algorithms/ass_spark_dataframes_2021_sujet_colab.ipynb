{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbuZfFWZVXpn"
   },
   "source": [
    "<h1><center>Big Data Algorithms Techniques & Platforms</center></h1>\n",
    "<h2>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Spark and DataFrames</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBLI_qUSVXpq"
   },
   "source": [
    "## Objectives \n",
    "\n",
    "<strong> Dataframes: </strong>\n",
    "<ul>\n",
    "    <li>  Pyspark </li> \n",
    "    <li>  Pandas library on Spark</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42q0jiKKVXpr"
   },
   "source": [
    "# A. Context\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "For running this serie of exercises we are going to use a quite big dataset containing data on Bitcoin made available from <a href=\"https://www.kaggle.com/mczielinski/bitcoin-historical-data\">Kaggle</a>.\n",
    "\n",
    "As stated in the description of the dataset:\n",
    "\"Bitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary.\" \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "### The dataset\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The dataset is in a .csv file:\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "\n",
    "CSV files for select bitcoin exchanges for the time period of Jan 2012 to December March 2021, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. \n",
    "\n",
    "Notice that:\n",
    "<ul>\n",
    "    <li> Timestamps are in Unix time.</li>\n",
    "<li> Timestamps without any trades or activity have their data fields filled with NaNs. </li>\n",
    "<li>  If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforeseen technical error in data reporting or gathering. </li>\n",
    "</ul>\n",
    "As stated by the authors \"all effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk\".\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd5y1A2BVXpr"
   },
   "source": [
    "# B. Environment set-up\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As first step you must include your dataset in your environment.\n",
    "\n",
    "You can folllow the procedure that includes Kaggle data into colab working folders or simply download and re-upload the file on your Colab space.\n",
    "\n",
    "\n",
    "$bitstampUSD\\_1-min\\_data\\_2012-01-01\\_to\\_2021-03-31.csv$\n",
    "    \n",
    "and upload it in the folder where your notebook is supposed to read the input.\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As second step you must prepare your environment running the following two cells that:\n",
    "<ul>\n",
    "    <li> Import the Pandas library.</li>\n",
    "<li> Set the Spark environment and return a SparkSession (acting as was acting the SparkContext in the previous exercises). </li>\n",
    "</ul>    \n",
    "    \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wVZaAkIgVXps"
   },
   "outputs": [],
   "source": [
    "# import of Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnIAJZrfVXps",
    "outputId": "7e0a19e5-c915-4845-9b88-5ae5f37490d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/14 21:30:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
    "# !tar zxvf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "#import of the SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "#inizialization of the Spark Session\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Assignment2\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#Start a Spark session on the local machine \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nOcFr4uSV03u"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sauraj-popos:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc1606d0190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdYKtVF_VXpt"
   },
   "source": [
    "## B.1  File import\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In this exercise the goal is to create a Spark DataFrame from the csv file in imput. \n",
    "\n",
    "Recall that in Spark DataFrame the type of the columns is very important for the definition of the internal data representation. \n",
    "    \n",
    "For this step you the target set of typed columns is the following one: \n",
    "<ul>\n",
    "    <li>    $Date\\_Time: Timestamp$ </li>\n",
    "     <li>   $Open: double$ </li>\n",
    "     <li>   $High: double$ </li>\n",
    "    <li>    $Low: double$ </li>\n",
    "    <li>    $Close: double$ </li>\n",
    "    <li>    $Volume\\_BTC: double$ </li>\n",
    "    <li>    $Volume\\_Currency: double$ </li>\n",
    "    <li>    $Weighted\\_Price: double$ </li>\n",
    "</ul>\n",
    "    \n",
    "We will arrive to define the schema in 3 guided steps described in the following sections.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Notice that the header of the $csv$ file contains the data description and that the simple import of the\n",
    "file treats the timestamp column as a String. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In data import you must check that:\n",
    "<ul>\n",
    "    <li>  the types of the imported data (the ones read from the file using the operation you choose) are equal to the types in the given schema</li>\n",
    "    <li>  the names of columns correspond (and make transofrmations if necessary). </li> \n",
    "</ul>\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQZN4xiqVXpt"
   },
   "source": [
    "### <strong> Exercise 1.</strong> First import (1 point)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Import the csv file in Spark DataFrame. If you have any doubt you can always refer to the Spark 3.1.1 documentation:\n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/3.1.1/\">Spark Reference Documentation</a>\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "imMeOHQmVXpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the command that creates (reads) a Spark DataFrame and stores the reference in the dfs variable\n",
    "from pyspark.sql.types import *\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "schema = StructType([\n",
    "    StructField(\"Timestamp\",IntegerType(), True),\n",
    "    StructField(\"Open\",DoubleType(), True),\n",
    "    StructField(\"High\",DoubleType(), True),\n",
    "    StructField(\"Low\",DoubleType(), True),\n",
    "    StructField(\"Close\",DoubleType(), True),\n",
    "    StructField(\"Volume_(BTC)\",DoubleType(), True),\n",
    "    StructField(\"Volume_(Currency)\",DoubleType(), True),\n",
    "    StructField(\"Weighted_Price\",DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "dfs = spark.read.csv(\"BTC.csv\", schema=schema,header=True)\n",
    "dfs.printSchema()\n",
    "# dfs['Timestamp'] = dfs['Timestamp'].astype(int)\n",
    "# dfs = spark.createDataFrame(dfs)\n",
    "# #'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "# #show the DataFrame schema\n",
    "# dfs.printSchema()\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double]</font>\n",
    "#\n",
    "# Notice that if you have something like:\n",
    "# DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]\n",
    "# you forgot a step: you did not include the schema of the columns\n",
    "#\n",
    "# Notice also that if you have:\n",
    "# DataFrame[Timestamp: string, Open: string, High: string, Low: string, Close: string, Volume_(BTC): string, Volume_(Currency): string, Weighted_Price: string]\n",
    "# you also forgot a step: the type of the Timestamp must be a String\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "| Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|  0.45558087|     2.0000000193|          4.39|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325318100| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "|1325318160| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
      "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "XTbhA1Aip4mM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='Timestamp', _c1='Open', _c2='High', _c3='Low', _c4='Close', _c5='Volume_(BTC)', _c6='Volume_(Currency)', _c7='Weighted_Price'),\n",
       " Row(_c0='1325317920', _c1='4.39', _c2='4.39', _c3='4.39', _c4='4.39', _c5='0.45558087', _c6='2.0000000193', _c7='4.39'),\n",
       " Row(_c0='1325317980', _c1='NaN', _c2='NaN', _c3='NaN', _c4='NaN', _c5='NaN', _c6='NaN', _c7='NaN'),\n",
       " Row(_c0='1325318040', _c1='NaN', _c2='NaN', _c3='NaN', _c4='NaN', _c5='NaN', _c6='NaN', _c7='NaN'),\n",
       " Row(_c0='1325318100', _c1='NaN', _c2='NaN', _c3='NaN', _c4='NaN', _c5='NaN', _c6='NaN', _c7='NaN')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following command is going to show 5 rows of the DataFrame\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkQ8aYX-VXpv"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look again at the target schema:\n",
    "    \n",
    "<ul>\n",
    "    <li>    $Date\\_Time: Timestamp$ </li>\n",
    "     <li>   $Open: double$ </li>\n",
    "     <li>   $High: double$ </li>\n",
    "    <li>    $Low: double$ </li>\n",
    "    <li>    $Close: double$ </li>\n",
    "    <li>    $Volume\\_BTC: double$ </li>\n",
    "    <li>    $Volume\\_Currency: double$ </li>\n",
    "    <li>    $Weighted\\_Price: double$ </li>\n",
    "</ul>\n",
    "    \n",
    "You notice that the import data has three problems with respect to the target schema:\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "    <li> the $Date\\_Time$ column is not present in the original file </li>\n",
    "    <li> there is an $int$ column $Timestamp$ that can be converted and transformed to a $Date$</li> \n",
    "    <li> some of the column names contain not required parentesis. </li>\n",
    "</ul>     \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUS9JWIcVXpv"
   },
   "source": [
    "### <strong> Exercise 2. </strong> Timestamp column (1 point)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Refine the import of the csv file and convert the \"timestamp\" column in the proper $Timestamp$ type:\n",
    "    <ul>\n",
    "        <li>   Create a new column <code>Date\\_Time</code> that is the conversion of the $String$ column $Timestamp$ in $Timestamp$ type  </li>\n",
    "</ul>\n",
    "The Dataframe are immutable structure, then your procedure will use a command (discussed in the slides) that will create a new Spark $DataFrame$ from the $dfs$ $DataFrame$ having a different schema. \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the timestamp column of the csv file and from the imported DataFrame \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Pj8BeALtVXpv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: integer (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_(BTC): double (nullable = true)\n",
      " |-- Volume_(Currency): double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with Date_Time column\n",
    "# and stores the reference in the dfsdt variable (it must be a DataFrame Spark with Date_Time column)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfsdt = dfs.withColumn('Date_Time', dfs['Timestamp'].cast(TimestampType()))\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfsdt.printSchema()\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "# DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#Volume_(BTC): double, Volume_(Currency): double, Weighted_Price: double, Date_Time: timestamp]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "iBcoKytNVXpw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Timestamp=1325317920, Open=4.39, High=4.39, Low=4.39, Close=4.39, Volume_(BTC)=0.45558087, Volume_(Currency)=2.0000000193, Weighted_Price=4.39, Date_Time=datetime.datetime(2011, 12, 31, 8, 52)),\n",
       " Row(Timestamp=1325317980, Open=nan, High=nan, Low=nan, Close=nan, Volume_(BTC)=nan, Volume_(Currency)=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 8, 53)),\n",
       " Row(Timestamp=1325318040, Open=nan, High=nan, Low=nan, Close=nan, Volume_(BTC)=nan, Volume_(Currency)=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 8, 54)),\n",
       " Row(Timestamp=1325318100, Open=nan, High=nan, Low=nan, Close=nan, Volume_(BTC)=nan, Volume_(Currency)=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 8, 55)),\n",
       " Row(Timestamp=1325318160, Open=nan, High=nan, Low=nan, Close=nan, Volume_(BTC)=nan, Volume_(Currency)=nan, Weighted_Price=nan, Date_Time=datetime.datetime(2011, 12, 31, 8, 56))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfsdt.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWEJTFojVXpw"
   },
   "source": [
    "### <strong> Exercise 3.</strong> Column names (2 points)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "As you can see from the output of the previous exercise the names of the columns still present some problems since there are some parentesis that are not required.\n",
    "    <ul>\n",
    "     <li> Remove the not required parentesis from the colum names </li>\n",
    "     <li> Hint: look at the documentation of DataFrame API and check the operation for column renaming </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "yRv_VbgYVXpw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, Volume_BTC: double, Volume_Currency: double, Weighted_Price: double, Date_Time: timestamp]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with correct names for all the columns\n",
    "# and store the reference in the dfscr variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "dfscr = (dfsdt.withColumnRenamed('Volume_(BTC)','Volume_BTC')\n",
    "         .withColumnRenamed('Volume_(Currency)','Volume_Currency'))\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "#show the DataFrame schema\n",
    "dfscr\n",
    "\n",
    "#######################\n",
    "# EXPECTED OUTPUT:\n",
    "#DataFrame[Timestamp: int, Open: double, High: double, Low: double, Close: double, \n",
    "#          Volume_BTC: double, Volume_Currency: double, Weighted_Price: double]\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "YU2SFo0UVXpw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 08:52:00|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:53:00|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:54:00|\n",
      "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:55:00|\n",
      "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:56:00|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show 5 rows of the DataFrame\n",
    "dfscr.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw7ktEJKVXpx"
   },
   "source": [
    "## B.2 DataFrame columns \n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "In this part of the exercise we are going continue to  modify in the Spark DataFrames.\n",
    "\n",
    "    \n",
    "Remember that using  PySpark, it's possible to access a DataFrame's columns either by attribute (<code>df.attributeName</code>) or by indexing <code>(df['attributeName'])</code>.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    \n",
    "Loook at the list of the functions to get familiar with the documentation: some functions that can be of help to manipulate the schema:\n",
    "    \n",
    "<ul>\n",
    "     <li>    <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>.  </li>\n",
    "</ul>    \n",
    "    \n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBgUaY_oVXpx"
   },
   "source": [
    "### <strong> Exercise 4.</strong>  Add two new columns to the DataFrame (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to extend the DataFrame with two other columns: given the $Date\\_Time$ column create two new columns ($Year$ and $Month$) that contain \n",
    "    <ul>\n",
    "     <li> the year </li>\n",
    "     <li> the month of the year </li>\n",
    "</ul>\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDH7YsSeVXpx"
   },
   "source": [
    "<p align=\"justify\">\n",
    "<font size=\"3\">    \n",
    "Look at the documentation of Spark functions and find the two functions that are convenient for this use case (hint: the name of the columns can help: <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\">Spark Functions</a>)\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "mpL3519cVXpx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the functions that you will use\n",
    "\n",
    "############## WRITE YOUR CODE HERE ##############\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "############## END OF THE EXERCISE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "zsehHsySVXpz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 08:52:00|2011|   12|\n",
      "|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:53:00|2011|   12|\n",
      "|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:54:00|2011|   12|\n",
      "|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:55:00|2011|   12|\n",
      "|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:56:00|2011|   12|\n",
      "+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new Data Frame Spark with the two additional columns\n",
    "# and store the reference in the dfsym variable (Data Frame Spark with Correct Names)\n",
    "\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "\n",
    "dfsym = dfscr.withColumn('Year', year('Date_Time')).withColumn('Month', month('Date_Time'))\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsym.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#| Timestamp|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|1325317920|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#|1325317980| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#|1325318040| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#|1325318100| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#|1325318160| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----------+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDMJbhTbVXpz"
   },
   "source": [
    "###  <strong>Exercise 5.</strong>  Drop Timestamp (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Finally we clean the schema and we can remove the the $Timestamp$ column.\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Qm8I1zhzVXp0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 08:52:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:53:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:54:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:55:00|2011|   12|\n",
      "| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 08:56:00|2011|   12|\n",
      "+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write the command that creates a new DataFrame Spark from the dfsym without the Timestamp column\n",
    "# and store the reference in the dfc variable (Data Frame Spark Clean)\n",
    "#'''############## WRITE YOUR CODE HERE ##############'''\n",
    "dfsc = dfsym.drop(\"TimeStamp\")\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "dfsc.show(5)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|Open|High| Low|Close|Volume_BTC|Volume_Currency|Weighted_Price|          Date_Time|Year|Month|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n",
    "#|4.39|4.39|4.39| 4.39|0.45558087|   2.0000000193|          4.39|2011-12-31 07:52:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:53:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:54:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:55:00|2011|   12|\n",
    "#| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011-12-31 07:56:00|2011|   12|\n",
    "#+----+----+----+-----+----------+---------------+--------------+-------------------+----+-----+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dm-ovIcVXp0"
   },
   "source": [
    "#  C. Using Parquet\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "In order to gain in performance in the following it is a good idea, as we have seen at lesson, to use a NoSQL structure, here Parquet, that will \n",
    "    allow \n",
    "to partition the SparkDataframe and to store it in multiple Parquet files. \n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WicAuWMfA9xo"
   },
   "source": [
    "## C.1 Saving data in Parquet\n",
    "    \n",
    "For this first example partition the file according to:\n",
    "    \n",
    " <ul>\n",
    "     <li> the year </li>\n",
    "             <li> the month of the year </li>\n",
    "</ul>\n",
    "The $partitionBy()$ operation can help for this step (Documentation of reference: <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">Spark Functions</a>).\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84UNDFHiVXp0"
   },
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "vTR8AxS2VXp0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to Parquet done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# here you can see and check the command that saves the dfsc DataFrame in Parquet\n",
    "\n",
    "dfsc.write.partitionBy([\"Year\", \"Month\"]).parquet(\"BTC/\",mode='overwrite')\n",
    "\n",
    "print(\"write to Parquet done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic-eLcr0VXp0"
   },
   "source": [
    "##  C.2 Check the folder Structure\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Look at the folder structure that has been created for the storage of the file. You see how the partitioning stategy of Parquet and the data distribution of Spark can be used, explicitely or implicitely, to improve performance.\n",
    "\n",
    "While you navigate (and the folder structure) data remember that in the data access:\n",
    "    \n",
    " <ul>\n",
    "     <li> the navigation is done using Parquet </li>\n",
    "     <li> the leaf contain the encoded Parquet files </li>\n",
    "</ul>\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC1C75uIVXp1"
   },
   "outputs": [],
   "source": [
    "#BTC\n",
    "#        ├── Year=2011\n",
    "#        │   ├── ...\n",
    "#        │   │\n",
    "#        │   ├── month=12\n",
    "#        ├── Year=2012\n",
    "#        │   ├── month=1\n",
    "#        │   ├── ...\n",
    "#        │   │\n",
    "#       ...\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JeyD03ipig_"
   },
   "source": [
    "This folder structure correspond to a phisical and logical data partition and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMCbaNcPVXp1"
   },
   "source": [
    "# D. Pandas\n",
    "\n",
    " \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "This data organization opens the opportunity to read data also using Pandas and not using Parquet.\n",
    "    \n",
    "Look at the documentation and check how you can read a Parquet structure and store it in a Pandas DataFrame:\n",
    "<a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html\">Pandas and Parquet</a>\n",
    "\n",
    "Notice how at the data-exchange base there is the presence of Arrow (thanks to $pyarrow$).\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Write the command that using Pandas read the data for the year 2011.\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "B11RsK9YVXp1"
   },
   "outputs": [],
   "source": [
    "#import of pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "jlRYuW0hVXp1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Volume_Currency</th>\n",
       "      <th>Weighted_Price</th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.39</td>\n",
       "      <td>2011-12-31 07:52:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:53:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:54:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:55:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 07:56:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 22:55:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 22:56:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 22:57:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 22:58:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-12-31 22:59:00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>908 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Open  High   Low  Close  Volume_BTC  Volume_Currency  Weighted_Price  \\\n",
       "0    4.39  4.39  4.39   4.39    0.455581              2.0            4.39   \n",
       "1     NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "2     NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "3     NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "4     NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "..    ...   ...   ...    ...         ...              ...             ...   \n",
       "903   NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "904   NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "905   NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "906   NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "907   NaN   NaN   NaN    NaN         NaN              NaN             NaN   \n",
       "\n",
       "              Date_Time Month  \n",
       "0   2011-12-31 07:52:00    12  \n",
       "1   2011-12-31 07:53:00    12  \n",
       "2   2011-12-31 07:54:00    12  \n",
       "3   2011-12-31 07:55:00    12  \n",
       "4   2011-12-31 07:56:00    12  \n",
       "..                  ...   ...  \n",
       "903 2011-12-31 22:55:00    12  \n",
       "904 2011-12-31 22:56:00    12  \n",
       "905 2011-12-31 22:57:00    12  \n",
       "906 2011-12-31 22:58:00    12  \n",
       "907 2011-12-31 22:59:00    12  \n",
       "\n",
       "[908 rows x 9 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we show you how we can create DataFrame using Pandas functions and reading from Parquet the data only for the year 2011/\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"BTC/Year=2011\")\n",
    "\n",
    "df\n",
    "#######################\n",
    "# Check the expected output:\n",
    "#Open\tHigh\tLow\tClose\tVolume_BTC\tVolume_Currency\tWeighted_Price\tDate_Time\tMonth\n",
    "#0\t4.39\t4.39\t4.39\t4.39\t0.455581\t2.0\t4.39\t2011-12-31 07:52:00\t12\n",
    "#1\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:53:00\t12\n",
    "#2\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:54:00\t12\n",
    "#3\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\tNaN\t2011-12-31 07:55:00\t12\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAYamKaEVXp1"
   },
   "source": [
    "###  D.1 Read Parquet file\n",
    "    \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Here you can see now the the Spark DataFrame is created from Parquet data.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "6C0iA8SqVXp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read done\n"
     ]
    }
   ],
   "source": [
    "# And here how we can create a DataFrame using Spark and reading the whole data/\n",
    "\n",
    "dfs = spark.read.parquet(\"BTC/\")\n",
    "\n",
    "print(\"read done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjH0cqC5VXp2"
   },
   "source": [
    "## <strong>Exercise 6</strong>. Verify number of column and count the number of rows (2 points)\n",
    "    \n",
    "<p align=\"justify\">\n",
    "Maybe you have not noticed that the volume of data we are treating is not so small as it seems. \n",
    "Count how many rows we are manipulating in the dataframe <code>dfs</code>\n",
    "<font size=\"3\">\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "nYqK9VD8VXp2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 4) / 5]\r",
      "\r",
      "[Stage 31:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4857377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the command that returns the number of rows of the DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "count = dfs.count()\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "print(count)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "# 4857377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "ZJVbEunCVXp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume_BTC: double (nullable = true)\n",
      " |-- Volume_Currency: double (nullable = true)\n",
      " |-- Weighted_Price: double (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can also check and verify the schema of the DataFrame\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoAfjdAuVXp3"
   },
   "source": [
    "# E. Statistics\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   Mean_Vol  : double </li>\n",
    "     <li>   Std_Vol   : double </li>\n",
    "     <li>   Min_Vol   : double </li>\n",
    "     <li>   Max_vol   : double </li>\n",
    "     <li>   Year      : int </li>\n",
    "     <li>   Month     : int </li>\n",
    "  \n",
    "</ul>\n",
    "\n",
    "In this exercise you will have two develop different methodologies to compute the statistics:\n",
    "<ul>\n",
    "    <li>   using the <code>applyInPandas()</code> Pyspark function and the Pandas functions </li>\n",
    "     <li>  only using the Pyspark functionnalities </li>\n",
    "</ul>\n",
    "The statistics computed should be stored in a Pandas DataFrame with both the two approaches.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gts_hy5HVXp4"
   },
   "source": [
    "## E.1. Spark applyinPandas\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "The solution with $applyinPandas$ \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "icjKlPHcVXp4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# the Python function that must be used. \n",
    "\n",
    "def compute_stats(key,df):\n",
    "    res = df[\"Volume_BTC\"].describe()\n",
    "\n",
    "    res_dict = {}\n",
    "    for index, value in res.items():\n",
    "\n",
    "        if index == \"mean\":\n",
    "            res_dict[\"Mean_Vol\"] = value\n",
    "        elif index == \"std\":\n",
    "            res_dict[\"Std_Vol\"] = value\n",
    "        elif index == \"min\":\n",
    "            res_dict[\"Min_Vol\"] = value\n",
    "        elif index == \"max\":\n",
    "            res_dict[\"Max_Vol\"] = value\n",
    "\n",
    "    final =  pd.DataFrame([res_dict])\n",
    "    final[\"Year\"]  = key[0]\n",
    "    final[\"Month\"] = key[1]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsqKcxxNVXp4"
   },
   "source": [
    "### <strong>Exercise 7</strong>. The two parameters of the Python function (2 points)\n",
    "The two parameters of the Python <code>applyinPandas(funct,schema)</code> function (2 points)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "    Look at the documentation of the <code>applyinPandas(funct,schema)</code> (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html\">click here to go to the documentation of <code>applyinpandas</code></a>) and describe how it works in detail from the DataFrame point of view in our example (what the $key$ and the $df$ will contain in our example).\n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99_WnUHTVXp5"
   },
   "source": [
    "#### WRITE YOUR ANSWER HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `applyinPandas` takes in a pandas dataframe and applies a given function to the schemas that have been specified in the function. Each column name in the schema acts as a key, with the columns assigned to each one of them, and whenever we wish to perform an operation on a column, the function applies the operations onto the schema-specified columns and returns a Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shVh5lBcVXp5"
   },
   "source": [
    "### <strong>Exercise 8</strong>. The two parameters in action (1 point)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Compute the statistics using then the $applyInPandas$ and the provided functions. \n",
    "\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "IebDul7NVXp6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:======================>                                  (8 + 4) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+------------+----+-----+\n",
      "|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
      "+------------------+------------------+----------+------------+----+-----+\n",
      "|20.352103880505254|54.288717926391755|    9.4E-5|2258.8231405|2012|   10|\n",
      "|12.194840033574591|  44.4857765359327| 2.0452E-4|2037.2239038|2015|    2|\n",
      "|6.1445201691912095|17.735804205800605|0.00127783|564.21436237|2019|   10|\n",
      "| 8.455668150732947|28.950141328303474|    1.0E-8|1616.0600006|2017|    3|\n",
      "|  8.68262839086814|17.696314028993037|       0.0|533.10078293|2017|    8|\n",
      "+------------------+------------------+----------+------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:==================================================>     (18 + 2) / 20]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = \"Mean_Vol double, Std_Vol double, Min_Vol double, Max_Vol double, Year int, Month int\"\n",
    "\n",
    "# Write the command that will store in the variable statsdf the DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "statsdf = dfs.groupby('Year','Month').applyInPandas(compute_stats,schema=schema)\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "statsdf.show(5)\n",
    "gc.collect()\n",
    "\n",
    "####### EXPECTED OUTPUT\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
    "#+------------------+------------------+----------+------------+----+-----+\n",
    "#| 20.39613620802532| 54.24699556644988|    9.4E-5|2258.8231405|2012|   10|\n",
    "#|12.095179597807542|44.149334198665166| 2.0452E-4|2037.2239038|2015|    2|\n",
    "#| 6.147061206279663|17.745599117954125|0.00127783|564.21436237|2019|   10|\n",
    "#| 8.468866447160776|  28.9837002907642|    1.0E-8|1616.0600006|2017|    3|\n",
    "#| 8.684880075589284| 17.69646210434965|       0.0|533.10078293|2017|    8|\n",
    "#+------------------+------------------+----------+------------+----+-----+#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(statsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6Pdf1N-VXp6"
   },
   "source": [
    "### <strong>Exercise 9</strong>. The statsdf DataFrame (1 point)\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Which kind of DataFrame is statsdf?\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbXKRuBEVXp6"
   },
   "source": [
    "#### WRITE YOUR ANSWER HERE ###\n",
    "\n",
    "**Spark dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgGmWPhSVXp7"
   },
   "source": [
    "\n",
    "### <strong>Exercise 10</strong>. DataFrame in Pandas (2 points)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Since we computed a stat by month the results will be small (we will have only one row by month)\n",
    "we can get and handle all the results in memory in Pandas.  \n",
    "    \n",
    "Notice that Spark is lazy so the $toPandas$ action will trigger the computation.\n",
    "    \n",
    "Write the command that will do the operation.\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "Wupi4CTgVXp7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.3 ms, sys: 4.08 ms, total: 58.4 ms\n",
      "Wall time: 8.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Write the command that will store in the variable stats_dfp the outoput DataFrame \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "stats_dfp = statsdf.toPandas()\n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Ve6u_nzSVXp8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Vol</th>\n",
       "      <th>Std_Vol</th>\n",
       "      <th>Min_Vol</th>\n",
       "      <th>Max_Vol</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.352104</td>\n",
       "      <td>54.288718</td>\n",
       "      <td>9.400000e-05</td>\n",
       "      <td>2258.823141</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.194840</td>\n",
       "      <td>44.485777</td>\n",
       "      <td>2.045200e-04</td>\n",
       "      <td>2037.223904</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.144520</td>\n",
       "      <td>17.735804</td>\n",
       "      <td>1.277830e-03</td>\n",
       "      <td>564.214362</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.455668</td>\n",
       "      <td>28.950141</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>1616.060001</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.682628</td>\n",
       "      <td>17.696314</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>533.100783</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.011746</td>\n",
       "      <td>57.570766</td>\n",
       "      <td>2.044000e-05</td>\n",
       "      <td>4111.876106</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.001140</td>\n",
       "      <td>18.931959</td>\n",
       "      <td>1.054000e-05</td>\n",
       "      <td>822.866974</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.330607</td>\n",
       "      <td>18.349660</td>\n",
       "      <td>5.758000e-04</td>\n",
       "      <td>806.636224</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.607801</td>\n",
       "      <td>18.802067</td>\n",
       "      <td>4.047000e-04</td>\n",
       "      <td>602.282607</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.108043</td>\n",
       "      <td>10.734858</td>\n",
       "      <td>3.300000e-06</td>\n",
       "      <td>582.564185</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean_Vol    Std_Vol       Min_Vol      Max_Vol  Year  Month\n",
       "0  20.352104  54.288718  9.400000e-05  2258.823141  2012     10\n",
       "1  12.194840  44.485777  2.045200e-04  2037.223904  2015      2\n",
       "2   6.144520  17.735804  1.277830e-03   564.214362  2019     10\n",
       "3   8.455668  28.950141  1.000000e-08  1616.060001  2017      3\n",
       "4   8.682628  17.696314  0.000000e+00   533.100783  2017      8\n",
       "5  16.011746  57.570766  2.044000e-05  4111.876106  2014      4\n",
       "6   5.001140  18.931959  1.054000e-05   822.866974  2020      6\n",
       "7   8.330607  18.349660  5.758000e-04   806.636224  2019      5\n",
       "8   8.607801  18.802067  4.047000e-04   602.282607  2017     10\n",
       "9   3.108043  10.734858  3.300000e-06   582.564185  2018     10"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results\n",
    "stats_dfp.head(10)\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#\tMean_Vol\tStd_Vol\tMin_Vol\tMax_Vol\tYear\tMonth\n",
    "#0\t20.396136\t54.246996\t9.400000e-05\t2258.823141\t2012\t10\n",
    "#1\t12.095180\t44.149334\t2.045200e-04\t2037.223904\t2015\t2\n",
    "#2\t6.147061\t17.745599\t1.277830e-03\t564.214362\t2019\t10\n",
    "#3\t8.468866\t28.983700\t1.000000e-08\t1616.060001\t2017\t3\n",
    "#4\t8.684880\t17.696462\t0.000000e+00\t533.100783\t2017\t8\n",
    "#5\t16.040933\t57.641501\t2.044000e-05\t4111.876106\t2014\t4\n",
    "#6\t4.984386\t18.903445\t1.054000e-05\t822.866974\t2020\t6\n",
    "#7\t8.331579\t18.350084\t5.758000e-04\t806.636224\t2019\t5\n",
    "#8\t8.621910\t18.820399\t4.047000e-04\t602.282607\t2017\t10\n",
    "#9\t3.106413\t10.738051\t3.300000e-06\t582.564185\t2018\t10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX4X2ioqVXp8"
   },
   "source": [
    "###  <strong>Exercise 11</strong>. Show the stats of the stats (1 point)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin by month for all the years.\n",
    "\n",
    "The computed statistics will be stored in a DataFrame having this schema\n",
    "<ul>\n",
    "     <li>   the min of the set min values </li>\n",
    "     <li>   the mean of the set of mean values </li>\n",
    "     <li>   ... </li> \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==============================>                         (11 + 4) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+------------+----+-----+\n",
      "|          Mean_Vol|           Std_Vol|   Min_Vol|     Max_Vol|Year|Month|\n",
      "+------------------+------------------+----------+------------+----+-----+\n",
      "|20.352103880505254|54.288717926391755|    9.4E-5|2258.8231405|2012|   10|\n",
      "|12.194840033574591|  44.4857765359327| 2.0452E-4|2037.2239038|2015|    2|\n",
      "|6.1445201691912095|17.735804205800605|0.00127783|564.21436237|2019|   10|\n",
      "| 8.455668150732947|28.950141328303474|    1.0E-8|1616.0600006|2017|    3|\n",
      "|  8.68262839086814|17.696314028993037|       0.0|533.10078293|2017|    8|\n",
      "+------------------+------------------+----------+------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "statsdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "4FFQDxOyVXp8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=================================================>    (182 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|summary|          Mean_Vol|           Std_Vol|             Min_Vol|           Max_Vol|              Year|             Month|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|  count|               112|               112|                 112|               112|               112|               112|\n",
      "|   mean|10.782377969937508|28.873422536262968| 0.00455668607142857|1067.2847720235718|2016.0892857142858| 6.428571428571429|\n",
      "| stddev| 6.486289802706909|18.106762305726285|0.043048171728398324| 895.8083462469303|2.7164947320662614|3.5252353718985097|\n",
      "|    min| 2.930563415850738| 6.499002418431784|                 0.0|       43.31219578|              2011|                 1|\n",
      "|    max|31.541007442469045|106.85984459660557|          0.45558087|      5853.8521659|              2021|                12|\n",
      "+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the command that will show and compute the stats on the numerical columns of the statsdf DataFrame\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "stats_of_stats = statsdf.describe()\n",
    "stats_of_stats.show()\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Expected output:\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|summary|          Mean_Vol|           Std_Vol|             Min_Vol|           Max_Vol|              Year|             Month|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n",
    "#|  count|               112|               112|                 112|               112|               112|               112|\n",
    "#|   mean|10.782191354847754|28.871463944232485|0.004551177678571...|1067.2847720235718|2016.0892857142858| 6.428571428571429|\n",
    "#| stddev| 6.488551661205522| 18.11344145463867|0.043048607639448476| 895.8083462469303|2.7164947320662614|3.5252353718985097|\n",
    "#|    min| 2.929999689326444| 6.490701567379118|                 0.0|       43.31219578|              2011|                 1|\n",
    "#|    max|31.504423573146152|106.97606692383131|          0.45558087|      5853.8521659|              2021|                12|\n",
    "#+-------+------------------+------------------+--------------------+------------------+------------------+------------------+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wtYR3CaVXp8"
   },
   "source": [
    "# F. Plotting and equivalence \n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to plot the resutls of the statistics by year and month (that will be in the $x$ orizontal axis of the plot). \n",
    "\n",
    "$Plotly$ will be used for the plotting\n",
    "    \n",
    "\n",
    "This provided version of the code is fully working in Python.\n",
    "    \n",
    "\n",
    "A Python routine converts the two columns $Year$ and $Month$ into a $DateTime$ column 'Date' (in order to plot the data in relation with the date).\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "5E1eQmpJVXp9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/sauraj/anaconda3/lib/python3.7/site-packages (from plotly) (1.16.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.3.1 tenacity-8.0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#install plotly and import the libraries\n",
    "\n",
    "!pip install plotly\n",
    "\n",
    "from plotly.offline import iplot,init_notebook_mode\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "wlyn4HLZVXp9"
   },
   "outputs": [],
   "source": [
    "#Helper function that converts the Year Month of our data into Date type\n",
    "\n",
    "def get_date_from_year_month(df):\n",
    "    df[\"Date\"] = pd.to_datetime(df['Year'].astype(str) + '-' + df['Month'].astype(str), format='%Y-%m')\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "mS9hH6hHVXp9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean_Vol</th>\n",
       "      <th>Std_Vol</th>\n",
       "      <th>Min_Vol</th>\n",
       "      <th>Max_Vol</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>23.829470</td>\n",
       "      <td>22.711133</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2011</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4.031777</td>\n",
       "      <td>6.740555</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>43.312196</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>8.340128</td>\n",
       "      <td>11.936996</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>92.654874</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>15.167503</td>\n",
       "      <td>26.493098</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>247.560124</td>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21.661193</td>\n",
       "      <td>36.374988</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>300.516098</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.688490</td>\n",
       "      <td>12.974280</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>360.652808</td>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2020-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.739396</td>\n",
       "      <td>10.431074</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>358.619032</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10.257201</td>\n",
       "      <td>16.946494</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>398.565948</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.963195</td>\n",
       "      <td>10.967455</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>248.504517</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3.559885</td>\n",
       "      <td>6.499002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>164.425570</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Mean_Vol    Std_Vol   Min_Vol     Max_Vol  Year  Month       Date\n",
       "48  23.829470  22.711133  0.455581   48.000000  2011     12 2011-12-01\n",
       "77   4.031777   6.740555  0.020000   43.312196  2012      1 2012-01-01\n",
       "82   8.340128  11.936996  0.003138   92.654874  2012      2 2012-02-01\n",
       "91  15.167503  26.493098  0.002096  247.560124  2012      3 2012-03-01\n",
       "16  21.661193  36.374988  0.001276  300.516098  2012      4 2012-04-01\n",
       "..        ...        ...       ...         ...   ...    ...        ...\n",
       "18   6.688490  12.974280  0.001542  360.652808  2020     11 2020-11-01\n",
       "46   5.739396  10.431074  0.001030  358.619032  2020     12 2020-12-01\n",
       "79  10.257201  16.946494  0.000002  398.565948  2021      1 2021-01-01\n",
       "64   5.963195  10.967455  0.000443  248.504517  2021      2 2021-02-01\n",
       "60   3.559885   6.499002  0.000016  164.425570  2021      3 2021-03-01\n",
       "\n",
       "[112 rows x 7 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this phase we need to sort by the date to allow parallelisation of shuffled the results\n",
    "\n",
    "stats_dfp = get_date_from_year_month(stats_dfp)    \n",
    "stats_dfp.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "UunBZ10kVXp-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.4.2.min.js\"></script>                <div id=\"06bfd29e-addf-4832-889d-5f5bafba8bea\" class=\"plotly-graph-div\" style=\"height:1000px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"06bfd29e-addf-4832-889d-5f5bafba8bea\")) {                    Plotly.newPlot(                        \"06bfd29e-addf-4832-889d-5f5bafba8bea\",                        [{\"type\":\"scatter\",\"x\":[\"2011-12-01T00:00:00\",\"2012-01-01T00:00:00\",\"2012-02-01T00:00:00\",\"2012-03-01T00:00:00\",\"2012-04-01T00:00:00\",\"2012-05-01T00:00:00\",\"2012-06-01T00:00:00\",\"2012-07-01T00:00:00\",\"2012-08-01T00:00:00\",\"2012-09-01T00:00:00\",\"2012-10-01T00:00:00\",\"2012-11-01T00:00:00\",\"2012-12-01T00:00:00\",\"2013-01-01T00:00:00\",\"2013-02-01T00:00:00\",\"2013-03-01T00:00:00\",\"2013-04-01T00:00:00\",\"2013-05-01T00:00:00\",\"2013-06-01T00:00:00\",\"2013-07-01T00:00:00\",\"2013-08-01T00:00:00\",\"2013-09-01T00:00:00\",\"2013-10-01T00:00:00\",\"2013-11-01T00:00:00\",\"2013-12-01T00:00:00\",\"2014-01-01T00:00:00\",\"2014-02-01T00:00:00\",\"2014-03-01T00:00:00\",\"2014-04-01T00:00:00\",\"2014-05-01T00:00:00\",\"2014-06-01T00:00:00\",\"2014-07-01T00:00:00\",\"2014-08-01T00:00:00\",\"2014-09-01T00:00:00\",\"2014-10-01T00:00:00\",\"2014-11-01T00:00:00\",\"2014-12-01T00:00:00\",\"2015-01-01T00:00:00\",\"2015-02-01T00:00:00\",\"2015-03-01T00:00:00\",\"2015-04-01T00:00:00\",\"2015-05-01T00:00:00\",\"2015-06-01T00:00:00\",\"2015-07-01T00:00:00\",\"2015-08-01T00:00:00\",\"2015-09-01T00:00:00\",\"2015-10-01T00:00:00\",\"2015-11-01T00:00:00\",\"2015-12-01T00:00:00\",\"2016-01-01T00:00:00\",\"2016-02-01T00:00:00\",\"2016-03-01T00:00:00\",\"2016-04-01T00:00:00\",\"2016-05-01T00:00:00\",\"2016-06-01T00:00:00\",\"2016-07-01T00:00:00\",\"2016-08-01T00:00:00\",\"2016-09-01T00:00:00\",\"2016-10-01T00:00:00\",\"2016-11-01T00:00:00\",\"2016-12-01T00:00:00\",\"2017-01-01T00:00:00\",\"2017-02-01T00:00:00\",\"2017-03-01T00:00:00\",\"2017-04-01T00:00:00\",\"2017-05-01T00:00:00\",\"2017-06-01T00:00:00\",\"2017-07-01T00:00:00\",\"2017-08-01T00:00:00\",\"2017-09-01T00:00:00\",\"2017-10-01T00:00:00\",\"2017-11-01T00:00:00\",\"2017-12-01T00:00:00\",\"2018-01-01T00:00:00\",\"2018-02-01T00:00:00\",\"2018-03-01T00:00:00\",\"2018-04-01T00:00:00\",\"2018-05-01T00:00:00\",\"2018-06-01T00:00:00\",\"2018-07-01T00:00:00\",\"2018-08-01T00:00:00\",\"2018-09-01T00:00:00\",\"2018-10-01T00:00:00\",\"2018-11-01T00:00:00\",\"2018-12-01T00:00:00\",\"2019-01-01T00:00:00\",\"2019-02-01T00:00:00\",\"2019-03-01T00:00:00\",\"2019-04-01T00:00:00\",\"2019-05-01T00:00:00\",\"2019-06-01T00:00:00\",\"2019-07-01T00:00:00\",\"2019-08-01T00:00:00\",\"2019-09-01T00:00:00\",\"2019-10-01T00:00:00\",\"2019-11-01T00:00:00\",\"2019-12-01T00:00:00\",\"2020-01-01T00:00:00\",\"2020-02-01T00:00:00\",\"2020-03-01T00:00:00\",\"2020-04-01T00:00:00\",\"2020-05-01T00:00:00\",\"2020-06-01T00:00:00\",\"2020-07-01T00:00:00\",\"2020-08-01T00:00:00\",\"2020-09-01T00:00:00\",\"2020-10-01T00:00:00\",\"2020-11-01T00:00:00\",\"2020-12-01T00:00:00\",\"2021-01-01T00:00:00\",\"2021-02-01T00:00:00\",\"2021-03-01T00:00:00\"],\"y\":[23.829469525,4.03177731714571,8.340128418817393,15.16750256294614,21.661192867858087,21.485268056516723,31.541007442469045,19.796181098680254,21.113690496939842,17.131046129153834,20.352103880505254,23.03198786601244,25.386235369176863,25.66917253176551,19.541312506206157,13.145030771592092,13.741576206010123,8.0134360404025,8.781815603058725,15.086137562043303,12.960768680847035,11.859202898821914,18.43746709053851,23.44736935335494,22.859848684818672,11.435445907911083,23.249858962209924,14.019427579620581,16.011745749334484,9.939251839962528,9.71182283029781,5.8475316611626456,9.85759561059762,10.267053766772245,16.366914817683654,13.12240018003678,8.912825336048904,24.89926255852769,12.194840033574591,9.62847200907816,7.91271468359413,6.748611964043118,7.569263757778192,12.975355839475874,14.245077082637899,20.07001003346207,22.945834363313523,25.681828044549643,11.978454240042643,8.617914426265886,6.963384652269653,4.672950124649634,3.6391050331003165,5.0487028153925975,9.234334200014906,4.728191286750803,5.30460086448607,3.51700172064199,3.8903077701159208,5.6751857027658374,5.678037714029938,9.349565817561786,6.631899898329978,8.455668150732947,4.677096941841448,10.922463531876076,10.75544399125949,10.435479043081868,8.68262839086814,12.218194389945783,8.60780112825081,11.327239098502249,12.631100381446572,10.436112100959313,14.400622220827142,10.791449909891176,8.871980471934998,7.079999116880998,5.724260632812388,5.478186358511732,5.599370704180334,4.109773746757943,3.108043265093624,8.79497916046872,8.474918630271505,5.142367578648134,5.4971291089174175,4.038306429599888,7.009036090612429,8.33060729353002,7.755869186714254,8.386548903818815,5.678424797667591,5.082595039511359,6.1445201691912095,4.37796719509253,2.930563415850738,4.562784343847883,4.086106725061016,10.895215108114849,7.0193551457776024,8.805934787706509,5.001139767990119,4.59312735910372,4.408439470248463,4.953952779933494,4.181852972618008,6.68849016984981,5.739395807319176,10.25720132342425,5.96319504416423,3.5598854135198406]}],                        {\"height\":1000,\"showlegend\":true,\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Average Volume by Month of BTC\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('06bfd29e-addf-4832-889d-5f5bafba8bea');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING OF THE MEAN VOLUME BY MONTH\n",
    "mean_vol_trac = {\n",
    "    \"x\": stats_dfp.Date,\n",
    "    \"y\": stats_dfp[\"Mean_Vol\"],\n",
    "}\n",
    "\n",
    "layout = {\n",
    "  \"height\":1000,\n",
    "  \"showlegend\": True, \n",
    "  \"title\": \"Average Volume by Month of BTC\",\n",
    "}\n",
    "\n",
    "fig = go.Figure(data=[mean_vol_trac], layout=layout)\n",
    "fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1XHlwrvVXp-"
   },
   "source": [
    "###  <strong> Exercise 12 </strong> - Compute the statistics using Pyspark (1 point)\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "We want to calculate the statistics of the bitcoin as we did before but using Pandas.\n",
    "\n",
    "The steps will be:\n",
    "<ul>\n",
    "     <li>   import data from Parquet in a Spark DataFrame </li>\n",
    "     <li>   remove null values </li>\n",
    "     <li>   perform the aggregation of the results </li> \n",
    "     <li>   convert the results to Pandas </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "zcS4_jawVXp_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 99 µs, total: 26.9 ms\n",
      "Wall time: 4.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:===========================================>          (162 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Count</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std.</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2011</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>23.829470</td>\n",
       "      <td>22.711133</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2011-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>501</td>\n",
       "      <td>4.031777</td>\n",
       "      <td>6.740555</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>43.312196</td>\n",
       "      <td>2012-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>575</td>\n",
       "      <td>8.340128</td>\n",
       "      <td>11.936996</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>92.654874</td>\n",
       "      <td>2012-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2012</td>\n",
       "      <td>3</td>\n",
       "      <td>594</td>\n",
       "      <td>15.167503</td>\n",
       "      <td>26.493098</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>247.560124</td>\n",
       "      <td>2012-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>761</td>\n",
       "      <td>21.661193</td>\n",
       "      <td>36.374988</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>300.516098</td>\n",
       "      <td>2012-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>43052</td>\n",
       "      <td>6.688490</td>\n",
       "      <td>12.974280</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>360.652808</td>\n",
       "      <td>2020-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>44613</td>\n",
       "      <td>5.739396</td>\n",
       "      <td>10.431074</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>358.619032</td>\n",
       "      <td>2020-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>44617</td>\n",
       "      <td>10.257201</td>\n",
       "      <td>16.946494</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>398.565948</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "      <td>40293</td>\n",
       "      <td>5.963195</td>\n",
       "      <td>10.967455</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>248.504517</td>\n",
       "      <td>2021-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>43175</td>\n",
       "      <td>3.559885</td>\n",
       "      <td>6.499002</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>164.425570</td>\n",
       "      <td>2021-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Month  Count       Mean       Std.       Min         Max       Date\n",
       "49  2011     12      4  23.829470  22.711133  0.455581   48.000000 2011-12-01\n",
       "77  2012      1    501   4.031777   6.740555  0.020000   43.312196 2012-01-01\n",
       "82  2012      2    575   8.340128  11.936996  0.003138   92.654874 2012-02-01\n",
       "91  2012      3    594  15.167503  26.493098  0.002096  247.560124 2012-03-01\n",
       "17  2012      4    761  21.661193  36.374988  0.001276  300.516098 2012-04-01\n",
       "..   ...    ...    ...        ...        ...       ...         ...        ...\n",
       "18  2020     11  43052   6.688490  12.974280  0.001542  360.652808 2020-11-01\n",
       "46  2020     12  44613   5.739396  10.431074  0.001030  358.619032 2020-12-01\n",
       "79  2021      1  44617  10.257201  16.946494  0.000002  398.565948 2021-01-01\n",
       "63  2021      2  40293   5.963195  10.967455  0.000443  248.504517 2021-02-01\n",
       "60  2021      3  43175   3.559885   6.499002  0.000016  164.425570 2021-03-01\n",
       "\n",
       "[112 rows x 8 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# solution to compute the statistics using pyspark function \n",
    "from pyspark.sql.functions import min, max, mean, stddev\n",
    "import pyspark.sql.functions as funcs\n",
    "\n",
    "\n",
    "# full spark dataframe (recall exercise 8a)\n",
    "df_spark = spark.read.parquet(\"BTC/\") \n",
    "\n",
    "# the na drop is important to be able to compute properly the stats\n",
    "# look at the documentation of the na.drop function\n",
    "group_ym = df_spark.na.drop().select([\"Volume_BTC\",\"Year\",\"Month\"]).groupBy([\"Year\",\"Month\"])\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "           \n",
    "    \n",
    "# aggregation \n",
    "# notice that the argument of the agg function is strictly related to min Vol, max Vol, mean, and stddev.\n",
    "res_df = group_ym.agg(funcs.count('Volume_BTC').alias('Count'),\n",
    "                     funcs.mean('Volume_BTC').alias('Mean'),\n",
    "                     funcs.stddev('Volume_BTC').alias('Std.'),\n",
    "                     funcs.min('Volume_BTC').alias('Min'),\n",
    "                     funcs.max('Volume_BTC').alias('Max'))\n",
    "\n",
    "\n",
    "#conversion the results to pandas\n",
    "stats_dfs = res_df.toPandas()\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n",
    "stats_dfs = get_date_from_year_month(stats_dfs)    \n",
    "stats_dfs.sort_values(by = 'Date',inplace = True)\n",
    "stats_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAzvUAQJVXp_"
   },
   "source": [
    "### Extra. Equivalence of results\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen the two procedures to get the results you must compare the outputs:\n",
    "<ul>\n",
    "     <li>   verify if the pandas dataframe from applyInPandas and PySpark functions are equivalents (look at the documentation to find the function that asserts if two DataFrames are equals) </li> \n",
    "         <li> compare the processing time between applyInPandas and PySpark routine with functions (that we have visualised with the %%time function) and comment them.</li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lngxwISVXp_"
   },
   "outputs": [],
   "source": [
    "# verify if the pandas dataframe from applyInPands and PySpark functions are equivalents \n",
    "# compare the processing time between applyInPandas and PySpark function\n",
    "cols = ['Mean_Vol', 'Std_Vol', 'Min_Vol', 'Max_Vol', 'Year', 'Month', 'Date']\n",
    " \n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "# equivalence verification: look at the pandas API and check if there is any test/assertion operation of help\n",
    "\n",
    "\n",
    "# comment about time execution and draw your considerations\n",
    "#'''############## END OF THE EXERCISE ##############'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSw_cXcmVXqA"
   },
   "source": [
    "### Extra - Plotting the financial data (1 point)\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Now that you have seen some examples you can draw your graphs:\n",
    "<ul>\n",
    "     <li>   filter the global data frame fron Parquet and take only the first day of the year 2021 </li> \n",
    "         <li> convert it to a pandas dataframe </li> \n",
    "         <li>    display the data using the $plot_candlestick$ routine </li> \n",
    " \n",
    "</ul>\n",
    "\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R05figbfVXqA"
   },
   "outputs": [],
   "source": [
    "#this function helps you to display the candlestick ( representation of financial data) of the pandas dataframe\n",
    "\n",
    "def plot_candlestick(df):\n",
    "    trace = {\n",
    "      \"x\": df.Date_Time,\n",
    "      \"close\": dfp[\"Open\"],\n",
    "      \"decreasing\": {\"line\": {\"color\": \"#008000\"}}, \n",
    "      \"high\":df[\"High\"] ,\n",
    "      \"increasing\": {\"line\": {\"color\": \"#db4052\"}}, \n",
    "      \"low\": df[\"Low\"],\n",
    "      \"name\": \"BTC\", \n",
    "      \"open\": df[\"Close\"],\n",
    "      \"type\": \"candlestick\"\n",
    "    }\n",
    "\n",
    "    layout = {\n",
    "      \"height\":1000,\n",
    "      \"showlegend\": True, \n",
    "      \"title\": \"Technical Analysis\",\n",
    "    }\n",
    "    \n",
    "    fig = go.Figure(data=[trace], layout=layout)\n",
    "    fig.show(renderer=\"colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hu81JTxBVXqA"
   },
   "outputs": [],
   "source": [
    "# Exercise filter the spark dataframe by date \n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "#read the global dataframe (as usual)\n",
    "df_spark = \n",
    "\n",
    "#create the beginning end date \n",
    "beg = \n",
    "end = \n",
    "\n",
    "\n",
    "#create the filter\n",
    "df_spark_filtered = \n",
    "\n",
    "#apply and convert it to pandas\n",
    "dfp = \n",
    "#'''############## END OF THE EXERCISE ##############'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5VQE8mhPVXqB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_candlestick(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxGOfykpVXqB"
   },
   "source": [
    "####  Extra - Propose your analysis\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "Think about a new analysis on this set of data to run on your data and run it showing a graph\n",
    "</font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewmVzLeuVXqB"
   },
   "outputs": [],
   "source": [
    "#'''############## WRITE YOUR ANSWER HERE ##############'''\n",
    "\n",
    "#'''############## END OF THE EXERCISE ##############'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524h_jgeVXqB"
   },
   "source": [
    "### Conclusion \n",
    "\n",
    "<p align=\"justify\">\n",
    "<font size=\"3\">\n",
    "$ApplyinPandas$ can be very powerful when you need to apply advanced Python code or Python libraries (i.e. <a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>  otherwise you can use Pyspark routines relying on most powerful storage techniques for example using Parquet.\n",
    "\n",
    "    \n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eG3YxgAzVXqB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ass_spark_dataframes_2021_sujet_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
